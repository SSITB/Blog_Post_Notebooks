{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Spark Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easiest way to make an RDD is to use the `parallelize` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see that we have a special type of RDD - known as the Parallel Collection RDD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD is very useful for dealing with failures when you are spreading jobs out to many clusters. The R in RDD means resilient - this means that all the data can be reconstructed on the fly should a node fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports reading text files from any Hadoop system like HDFS or a local file system.\n",
    "\n",
    "Be sure that the file is accessible over the network or is in the same place for all of your worker nodes to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "text = sc.textFile(os.getcwd()+\"/text_files/harvard_sentences.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that we have a Map Partitions RDD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oak is strong and also gives shade.',\n",
       " 'Cats and dogs each hate the other.',\n",
       " 'The pipe began to rust while new.',\n",
       " \"Open the crate but don't break the glass.\",\n",
       " 'Add the sum to the product of these three.',\n",
       " 'Thieves who rob friends deserve jail.',\n",
       " 'The ripe taste of cheese improves with age.',\n",
       " 'Act on these orders with great speed.',\n",
       " 'The hog crawled under the high fence.',\n",
       " 'Move the vat over the hot fire.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun fact - the above sentences are known as [Harvard Sentences](https://en.wikipedia.org/wiki/Harvard_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sc.textFile` has an optional argument `minPartitions=None`\n",
    "\n",
    "This argument tells spark how many partitions our data should have.\n",
    "\n",
    "You should note that the minPartitons is equivalent to the amount of parallelism that you will employ.\n",
    "\n",
    "That is, `minPartitions = 1` means that only 1 node/executor will be processing your data. This doesnt exactly make sense when dealing with a cluster - you want to partition your data out amongst several nodes and then let each executor run your code in parallel on the data.\n",
    "\n",
    "Furthermore, you are specifying the MINIMUM number of partitions, as such, this wioll serve as a the lower bound on your parallelism. Spark may decide to give you more, i.e. 5\n",
    "\n",
    "Now, ifg you had 15 nodes - you may be tempted to set `minPartitions = 15` - indeed, this is exactly what you should do as you will ensure every node will get some data to work with.\n",
    "\n",
    "However, in our incredibly simplistic example above, we only have 10 sentences, thus setting `minPartitions = 15` doesnt make much sense, as there is no way to split this data 15 ways. Spark is NOT going to split up a sentence.\n",
    "\n",
    "Another thing to note is that when loading data from HDFS, spark will assign one partition per block - if I recall correctly, a block size is 64MB by default.\n",
    "\n",
    "Spark docs recommend 2-4 partitions per CPU on your machine.\n",
    "\n",
    "The other optional argument is `use_unicode=True` - if you are certain that your text data contains no unicode characters, then setting this parameter to False will make your data utf-8. This is a performance improvement, so be sure to use it where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark 'Actions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think of 'actions' as poutput producing functions - they force spark to do a computation and spit out a result.\n",
    "\n",
    "In terms of inputs and outputs, if a function takes in an RDD and spits out something that isnt an RDD - it is an action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.collect()` pulls the result into the driver. By driver, I mean the application in which the result is being computed and displayed. In this case, that is a jupyter notebook. In other cases, it could be a spark shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(4) # takes the first x elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() # equivalent of len(list(range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numbers.saveAsTextFile(\"sample_numbers.txt\") - saves to disk as .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark 'Transformations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations allow us to define computations on RDD's.\n",
    "\n",
    "In terms of inputs and outputs, a function that takes an RDD and putputs an RDD is a transformation.\n",
    "\n",
    "It is important to note that the output of one transformation can serve as the input for another transformation.\n",
    "\n",
    "This way, we are building up a tree or **graph** of transformations that need to be applied to out initial RDD. No computation occurs whatsoever when specifying transformations - spark is **lazy**.\n",
    "\n",
    "In fact, when you perform an action, spark applies a transformation to RDD's in a recursive fashion until it reaches an RDD that originates from an input source. This stop the recursion and allows all of the transformations to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_rdd = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'heritage' of an RDD can be foundusing the `toDebugString()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(8) PythonRDD[12] at collect at <ipython-input-14-e0ebabccc164>:1 []\\n |  ParallelCollectionRDD[10] at parallelize at PythonRDD.scala:489 []'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_heritage(rdd):\n",
    "    for s in rdd.toDebugString().split(b'\\n'): #it's a bytes object.\n",
    "        print(s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combo = rdd.union(small_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(16) UnionRDD[13] at union at NativeMethodAccessorImpl.java:0 []'\n",
      "b'|   PythonRDD[11] at collect at <ipython-input-13-20868699513c>:1 []'\n",
      "b'|   ParallelCollectionRDD[9] at parallelize at PythonRDD.scala:489 []'\n",
      "b'|   PythonRDD[12] at collect at <ipython-input-14-e0ebabccc164>:1 []'\n",
      "b'|   ParallelCollectionRDD[10] at parallelize at PythonRDD.scala:489 []'\n"
     ]
    }
   ],
   "source": [
    "show_heritage(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combo is an RDD - however, when looking at its 'heritage' we see the information of `numbers` and `subset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Persistence\n",
    "\n",
    "The general idea here is that when we do'expensive' transformations on RDD's, we may want to keep that RDD around - i.e. in 'memory'.\n",
    "\n",
    "By default, spark stores the RDD in memory, persisting an RDD allows you to either fully store it in memory, fully store it on disk or use a combination of the two. Furthermore, we know that when we save to disk, the data is serialized - however, you can also serialize the data **in memory**. You can specify the level of storage for persistance.\n",
    "\n",
    "Furthermore, you can set a _replication level_ for the persistance. The higher the replication, the higher 'insurance' you have on replicating all your data if you lose a node/executor. In addition, higher replciation means that more processes can be run on the data if the data in question is being used as a source.\n",
    "\n",
    "As such, by persisting an RDD, any subsequent action on that partiuclar RDD **within the same session** will **not** have to recurse all the way back to an input RDD.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2 - see more [here](https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#rdd-persistence)\n",
    "\n",
    "Also - once you have set a level of storage for an RDD, you cannot somply change the level, you must refresh the notebook, and set the new level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as pyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_numbers = sc.parallelize(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = big_numbers.map(lambda x : x ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans.saveAsTextFile(\"initial-trans.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans.map(lambda x: x - 22000).saveAsTextFile(\"second-trans.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a classic example of why we would prefer to persist an RDD. the second transformation, would require us to do all the transformations again. However, if we persisted the RDD, the second transformation would occur much faster since it no longer needs to recurse to the input RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans.persist(pyk.StorageLevel.MEMORY_AND_DISK) # this should occur before our first saveAsTextFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans.is_cached # will return True if RDD has been persisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trans.unpersist() # will unpersist after you are done with the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Map\n",
    "\n",
    "Works exactly the same as the built-in python version of map.\n",
    "\n",
    "Start with an RDD, map a function over that RDD!\n",
    "\n",
    "When using map, you should expect one input (RDD) and expect one output (RDD). Map creates an output for EACH input - where an input is every element present in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x * 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda syntax is quite common in PySpark, however, you can still define regular functions just like you would in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mult_2(val):\n",
    "    return 2 * val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(mult_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when writing traditional functions like this, it is important that this function is a **pure** function - that is, it doesnt alter/store/reference  any states/variables etc outside of its scope.\n",
    "\n",
    "This is incredibly important as we are doing distributed work across multiple executors. As such, we must be sure that given an input, our function will always give the same output. This is an identitcal concept to [**referential transparency**](https://wiki.haskell.org/Referential_transparency) employed by functional programming languages i.e. Haskell.\n",
    "\n",
    "Map has an optional argument `preservesPartitioning=False`. In essence, if set to `True`, it will ensure that the mapping over your data does not change the way your data was paritioned. This is a more advanced spark feature, however, it is incredibly useful when doing joins! In general, if yu wish to preserve partition strucutre, check out the `mapValues` function instead of `map`.\n",
    "\n",
    "`mapValues` requires a Key-Value pair, it preserves the keys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "\n",
    "def f(val):\n",
    "    return len(val)\n",
    "\n",
    "x.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Filter\n",
    "\n",
    "You can think of filter as being equivalent to a `WHERE` clause in SQL. It only keeps the values that are relevent in your RDD!\n",
    "\n",
    "Again, each element in the RDD is passed through the filter. If the value passng through the function evaluates to True, the value is kept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def even(val):\n",
    "    return val % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(even).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now - given a large dataset, say taht we filter out a significant chunk of the data. It is wise at this point to use the `coalesce` function.\n",
    "\n",
    "This function reduced the number of partitions on the resulting RDD in addition to **minimizing** network traffic.\n",
    "\n",
    "You should think of network traffic as **computation overhead**. That is, if you had a really large dataset that required 1000 Nodes - but now after filtering, all your need is 10 nodes. It doesnt make sense to start all 1000 nodes everytime you start manipulating an RDD that requires 10 Nodes.\n",
    "\n",
    "`coalesce` does this reduction for you! We will get to an example later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FlatMap\n",
    "\n",
    "Most beginners confuse this with `map` - dont do this!\n",
    "\n",
    "At a high level, `map` is a **one-to-one** transformation.\n",
    "\n",
    "`flatMap` is a **one-to-many** transformation!\n",
    "\n",
    "Think of flatmap as taking one input at a time, and for each input, producing many outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(os.getcwd()+\"/text_files/harvard_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oak',\n",
       " 'is',\n",
       " 'strong',\n",
       " 'and',\n",
       " 'also',\n",
       " 'gives',\n",
       " 'shade.',\n",
       " 'Cats',\n",
       " 'and',\n",
       " 'dogs',\n",
       " 'each',\n",
       " 'hate',\n",
       " 'the',\n",
       " 'other.',\n",
       " 'The',\n",
       " 'pipe',\n",
       " 'began',\n",
       " 'to',\n",
       " 'rust',\n",
       " 'while',\n",
       " 'new.',\n",
       " 'Open',\n",
       " 'the',\n",
       " 'crate',\n",
       " 'but',\n",
       " \"don't\",\n",
       " 'break',\n",
       " 'the',\n",
       " 'glass.',\n",
       " 'Add',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'to',\n",
       " 'the',\n",
       " 'product',\n",
       " 'of',\n",
       " 'these',\n",
       " 'three.',\n",
       " 'Thieves',\n",
       " 'who',\n",
       " 'rob',\n",
       " 'friends',\n",
       " 'deserve',\n",
       " 'jail.',\n",
       " 'The',\n",
       " 'ripe',\n",
       " 'taste',\n",
       " 'of',\n",
       " 'cheese',\n",
       " 'improves',\n",
       " 'with',\n",
       " 'age.',\n",
       " 'Act',\n",
       " 'on',\n",
       " 'these',\n",
       " 'orders',\n",
       " 'with',\n",
       " 'great',\n",
       " 'speed.',\n",
       " 'The',\n",
       " 'hog',\n",
       " 'crawled',\n",
       " 'under',\n",
       " 'the',\n",
       " 'high',\n",
       " 'fence.',\n",
       " 'Move',\n",
       " 'the',\n",
       " 'vat',\n",
       " 'over',\n",
       " 'the',\n",
       " 'hot',\n",
       " 'fire.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Oak', 'is', 'strong', 'and', 'also', 'gives', 'shade.'],\n",
       " ['Cats', 'and', 'dogs', 'each', 'hate', 'the', 'other.'],\n",
       " ['The', 'pipe', 'began', 'to', 'rust', 'while', 'new.'],\n",
       " ['Open', 'the', 'crate', 'but', \"don't\", 'break', 'the', 'glass.'],\n",
       " ['Add', 'the', 'sum', 'to', 'the', 'product', 'of', 'these', 'three.'],\n",
       " ['Thieves', 'who', 'rob', 'friends', 'deserve', 'jail.'],\n",
       " ['The', 'ripe', 'taste', 'of', 'cheese', 'improves', 'with', 'age.'],\n",
       " ['Act', 'on', 'these', 'orders', 'with', 'great', 'speed.'],\n",
       " ['The', 'hog', 'crawled', 'under', 'the', 'high', 'fence.'],\n",
       " ['Move', 'the', 'vat', 'over', 'the', 'hot', 'fire.']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you see the differnce! `flatMap` gives you access to all the words i all sentences directly! In essence, `flatMap` removes one level of grouping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MapPartitions\n",
    "\n",
    "Exactly the same as `map` however, it runs transformations over the partitions of an RDD and then aggregates thme together!\n",
    "\n",
    "Below I will show you an example of counting all the word occurences in our harvard sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(os.getcwd()+\"/text_files/harvard_sentences.txt\", minPartitions=7)\n",
    "all_words = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the iterator_obj is all of the values in a specific partition in the RDD.\n",
    "\n",
    "Since this behaves as an iterator object, we use the `yield` keyword and not `return`.\n",
    "\n",
    "This is essential, because we want to update counts as we go through each element in a partition rather than just returning one large ditcionary with everything at the end.\n",
    "\n",
    "An excellent explanation of Generators, `yield` and etc can be found [here](https://pythontips.com/2013/09/29/the-python-yield-keyword-explained/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_count(iterator_obj):\n",
    "    counts = {}\n",
    "    \n",
    "    for word in iterator_obj:\n",
    "        if word not in counts:\n",
    "            counts[word] = 1\n",
    "        else:\n",
    "            counts[word] +=1\n",
    "    \n",
    "    yield counts # we yield and NOT return here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = all_words.mapPartitions(generate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Cats': 1,\n",
       "  'Oak': 1,\n",
       "  'also': 1,\n",
       "  'and': 2,\n",
       "  'dogs': 1,\n",
       "  'each': 1,\n",
       "  'gives': 1,\n",
       "  'hate': 1,\n",
       "  'is': 1,\n",
       "  'other.': 1,\n",
       "  'shade.': 1,\n",
       "  'strong': 1,\n",
       "  'the': 1},\n",
       " {'Open': 1,\n",
       "  'The': 1,\n",
       "  'began': 1,\n",
       "  'break': 1,\n",
       "  'but': 1,\n",
       "  'crate': 1,\n",
       "  \"don't\": 1,\n",
       "  'glass.': 1,\n",
       "  'new.': 1,\n",
       "  'pipe': 1,\n",
       "  'rust': 1,\n",
       "  'the': 2,\n",
       "  'to': 1,\n",
       "  'while': 1},\n",
       " {'Add': 1,\n",
       "  'of': 1,\n",
       "  'product': 1,\n",
       "  'sum': 1,\n",
       "  'the': 2,\n",
       "  'these': 1,\n",
       "  'three.': 1,\n",
       "  'to': 1},\n",
       " {'Thieves': 1, 'deserve': 1, 'friends': 1, 'jail.': 1, 'rob': 1, 'who': 1},\n",
       " {'The': 1,\n",
       "  'age.': 1,\n",
       "  'cheese': 1,\n",
       "  'improves': 1,\n",
       "  'of': 1,\n",
       "  'ripe': 1,\n",
       "  'taste': 1,\n",
       "  'with': 1},\n",
       " {'Act': 1,\n",
       "  'The': 1,\n",
       "  'crawled': 1,\n",
       "  'fence.': 1,\n",
       "  'great': 1,\n",
       "  'high': 1,\n",
       "  'hog': 1,\n",
       "  'on': 1,\n",
       "  'orders': 1,\n",
       "  'speed.': 1,\n",
       "  'the': 1,\n",
       "  'these': 1,\n",
       "  'under': 1,\n",
       "  'with': 1},\n",
       " {'Move': 1, 'fire.': 1, 'hot': 1, 'over': 1, 'the': 2, 'vat': 1}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Cats': 1,\n",
       "  'Oak': 1,\n",
       "  'also': 1,\n",
       "  'and': 2,\n",
       "  'dogs': 1,\n",
       "  'each': 1,\n",
       "  'gives': 1,\n",
       "  'hate': 1,\n",
       "  'is': 1,\n",
       "  'other.': 1,\n",
       "  'shade.': 1,\n",
       "  'strong': 1,\n",
       "  'the': 1},\n",
       " {'Open': 1,\n",
       "  'The': 1,\n",
       "  'began': 1,\n",
       "  'break': 1,\n",
       "  'but': 1,\n",
       "  'crate': 1,\n",
       "  \"don't\": 1,\n",
       "  'glass.': 1,\n",
       "  'new.': 1,\n",
       "  'pipe': 1,\n",
       "  'rust': 1,\n",
       "  'the': 2,\n",
       "  'to': 1,\n",
       "  'while': 1},\n",
       " {'Add': 1,\n",
       "  'of': 1,\n",
       "  'product': 1,\n",
       "  'sum': 1,\n",
       "  'the': 2,\n",
       "  'these': 1,\n",
       "  'three.': 1,\n",
       "  'to': 1},\n",
       " {'Thieves': 1, 'deserve': 1, 'friends': 1, 'jail.': 1, 'rob': 1, 'who': 1},\n",
       " {'The': 1,\n",
       "  'age.': 1,\n",
       "  'cheese': 1,\n",
       "  'improves': 1,\n",
       "  'of': 1,\n",
       "  'ripe': 1,\n",
       "  'taste': 1,\n",
       "  'with': 1},\n",
       " {'Act': 1,\n",
       "  'The': 1,\n",
       "  'crawled': 1,\n",
       "  'fence.': 1,\n",
       "  'great': 1,\n",
       "  'high': 1,\n",
       "  'hog': 1,\n",
       "  'on': 1,\n",
       "  'orders': 1,\n",
       "  'speed.': 1,\n",
       "  'the': 1,\n",
       "  'these': 1,\n",
       "  'under': 1,\n",
       "  'with': 1},\n",
       " {'Move': 1, 'fire.': 1, 'hot': 1, 'over': 1, 'the': 2, 'vat': 1}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts.collect()) # output partitioned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted the index of the partition to be return in addition to the output, check out the `mapPartitionsWithIndex` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sample\n",
    "\n",
    "Returns a sample of your data.\n",
    "\n",
    "This sample can then be used for statistical analysis with regards to the population etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(99999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20020"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(False, 0.2).count() # 20% of data with No replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Union\n",
    "\n",
    "Allows you to combine RDD's - there is no remval of duplicates, sorting etc. It's simply a merging of the RDD's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = sc.parallelize(range(20))\n",
    "second = sc.parallelize(range(30))\n",
    "first.union(second).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Intersection\n",
    "\n",
    "Find elements that exist in both RDD's.\n",
    "\n",
    "Note: Intersection can be slow on very large databases as internally, spark is running a `reduce` job acorss multiple nodes. This data shifting between nodes and reduction can cause quite the overhead. As such, if you job is every very slow, check to see if you have used an intersection - can you optimize any further before using `intersection`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 5]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = sc.parallelize([1,2,3,4,4,5])\n",
    "second = sc.parallelize([5,17,20,4,1])\n",
    "first.intersection(second).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distinct\n",
    "\n",
    "Drops multiple duplciates from an RDD\n",
    "\n",
    "I will employ the `cartesian` method - this gives the [cartesian product](https://en.wikipedia.org/wiki/Cartesian_product)\n",
    "\n",
    "Recall that cartesian products scale with the size of your data = you should never be doing a cartesian product between 2 large data sets!\n",
    "\n",
    "Rather, you should find a way to take the product between smaller subsets. You can then broadcast your operations to the large dataset using `map`.\n",
    "\n",
    "If you must take the product of two large set - look into join, full outer join etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"Ibrahim\", \"Juan\"]).cartesian(sc.parallelize(range(25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ibrahim', 0),\n",
       " ('Ibrahim', 1),\n",
       " ('Ibrahim', 2),\n",
       " ('Ibrahim', 3),\n",
       " ('Ibrahim', 4),\n",
       " ('Ibrahim', 5),\n",
       " ('Ibrahim', 6),\n",
       " ('Ibrahim', 7),\n",
       " ('Ibrahim', 8),\n",
       " ('Ibrahim', 9),\n",
       " ('Ibrahim', 10),\n",
       " ('Ibrahim', 11),\n",
       " ('Ibrahim', 12),\n",
       " ('Ibrahim', 13),\n",
       " ('Ibrahim', 14),\n",
       " ('Ibrahim', 15),\n",
       " ('Ibrahim', 16),\n",
       " ('Ibrahim', 17),\n",
       " ('Ibrahim', 18),\n",
       " ('Ibrahim', 19),\n",
       " ('Ibrahim', 20),\n",
       " ('Ibrahim', 21),\n",
       " ('Ibrahim', 22),\n",
       " ('Ibrahim', 23),\n",
       " ('Ibrahim', 24),\n",
       " ('Juan', 0),\n",
       " ('Juan', 1),\n",
       " ('Juan', 2),\n",
       " ('Juan', 3),\n",
       " ('Juan', 4),\n",
       " ('Juan', 5),\n",
       " ('Juan', 6),\n",
       " ('Juan', 7),\n",
       " ('Juan', 8),\n",
       " ('Juan', 9),\n",
       " ('Juan', 10),\n",
       " ('Juan', 11),\n",
       " ('Juan', 12),\n",
       " ('Juan', 13),\n",
       " ('Juan', 14),\n",
       " ('Juan', 15),\n",
       " ('Juan', 16),\n",
       " ('Juan', 17),\n",
       " ('Juan', 18),\n",
       " ('Juan', 19),\n",
       " ('Juan', 20),\n",
       " ('Juan', 21),\n",
       " ('Juan', 22),\n",
       " ('Juan', 23),\n",
       " ('Juan', 24)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we needed to see how mnay unique names were in this list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Juan', 'Ibrahim']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x[0]).distinct().collect() # Awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.distinct()` takes an optional argument `numPartitions=None` - the higher you set this, the more parallelized your code will run. Again, `distinct` uses `reduce` behind the scenes, as such, performance can be slow with large amounts of data. Optimize as much as you can before using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pipe\n",
    "\n",
    "This function takes each partition of data within an RDD and **pipes** it to a command line tool of your choice!\n",
    "\n",
    "This is useful if you have already developed a command line tool in **ANY** language and now wish to parallelize it.\n",
    "\n",
    "All data is fed in a strings and output as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4',\n",
       " '14',\n",
       " '24',\n",
       " '34',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,50))\n",
    "rdd.pipe('grep 4').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coalesce\n",
    "\n",
    "This is an incredibly useful function - however, in order to really reap the benefits we must first dig a little deeper into how spark works under the hood.\n",
    "\n",
    "By now, you should know that Spark stores your data in a distrbuted manner. This means that data is split into chunks where each chunk is known as a **partitions**.Partitions exist throughout your entire cluster.\n",
    "\n",
    "In essence, the the coalesce functions allows you to **reduce** the number of partitions you have of your data in a **significantly more efficient** manner over doing a full repartition.\n",
    "\n",
    "How? Coalesce combines paritions that are **already on the same executors**. This minimizes network traffic **between** executors!\n",
    "\n",
    "This is all well and good, however, when do I need to change the number of paritions? What is the appropriate number of partitions?\n",
    "\n",
    "I like to think that choosing the optimal number of partitions is a competition between 2 important considerations:\n",
    "\n",
    "- The number of partitions is the **upper limit** for parallelism.\n",
    "    - This means its **impossible** to have 8 processors working on 3 partitions.Spark docs [recommend 2-4 tasks](https://spark.apache.org/docs/latest/tuning.html#level-of-parallelism) (aka partitions) per CPU in your cluster.\n",
    "    - Too many partions will also cause excessive network traffic due to the creation of many small tasks.\n",
    "\n",
    "- Number of partitions determines number of output files for an action.\n",
    "\n",
    "I would suggest using the Spark docs suggestions of 2-4 partitions/CPu as a rule of thumb.\n",
    "\n",
    "However, I cannot stress enough the importance of experimentation when it comes to tuning your settings.\n",
    "\n",
    "After running a job, cut the number of partitions in half and see what happens? Increase the number of partitions slightly, did it get faster?\n",
    "\n",
    "As we see in Machine Learning, most (Hyper)parameter tuning starts with an educated guess and then embarks on a series of iterations until we find that sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(99999), numSlices=1000) #original with 1000 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducedRDD = rdd.coalesce(30) #same rdd, but with 30 partitons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Repartition.\n",
    "\n",
    "A very useful function for when you want to increase the number of partions that your RDD started with. This can cause some overhead as new partitions are created and sent to nodes.\n",
    "\n",
    "If reducing the number of partitions - its better to use `coalesce` as this will minimize network overhead by merging together partitions that are already on the same node.\n",
    "\n",
    "Partitioning data is central to spark - this is something that just takes practice and some experimenting!\n",
    "\n",
    "Recall, 2-4 partions per CPU is a good rule of thumb to start at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(999), numSlices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[63] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(30) # simple as that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RepartitionAndSortWithinPartitions\n",
    "\n",
    "Same as the above function, but will also allow you to sort each partion there and then.\n",
    "\n",
    "This funciton works on key-value pairs and partitions the data according to key. This function is more efficent than just repartitioning, and then later on sorting the data.\n",
    "\n",
    "Learn about `.glom()` [here](https://spark.apache.org/docs/0.7.2/api/pyspark/pyspark.rdd.RDD-class.html#glom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([[3,41], [3,7], [100,100], [0.5, 17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.5, 17), (100, 100)], [(3, 41), (3, 7)]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartitionAndSortWithinPartitions(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outermost list containing everything is due to glom.\n",
    "\n",
    "Then we have two inner lists - this is because we repartitioned into 2. Each of those sublists is also sorted!\n",
    "\n",
    "Notie how the two 3 appears together - this means our sorting falls within our partioning!\n",
    "\n",
    "You can also specify a custom `partitionFunc` in the optional arguments.\n",
    "\n",
    "There is also the optional `ascending` and `keyFunc` arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.5, 17), (3, 41), (3, 7)], [(100, 100)]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartitionAndSortWithinPartitions(2, partitionFunc=lambda x: x == 100).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I specified that I want one partion to contain all keys that equal 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce\n",
    "\n",
    "Calculates aggregates **over many** inputs!\n",
    "\n",
    "Requires an **associative** and **commutative** function that is applied to pairs of inputs. The function is then applied between pairs and so on until we have a single output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [16, 17, 18, 19, 20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(25), numSlices=3)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is running the reduce command in parallel over every partition - think of this as asking for the max of the first sub list and so on.\n",
    "\n",
    "Then taking those results and asking for the max in that secondary comparison. Eventually, the answer **reduces** to 24.\n",
    "\n",
    "`reduceByKey` allows you to calculate aggregates based on subsets of data - like a pandas groupby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda x,y : x + y) # equivalent to sum(list(range(25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Count\n",
    "\n",
    "This function returns the number of elements in the RDD.\n",
    "\n",
    "More intersting are 2 other functions that are closely related:\n",
    "\n",
    "- `countApprox(timeout=500, confidence = 0.7)`\n",
    "\n",
    "The above function will use the [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog) algorithm to approximate the size of your data. This come in very handy when you have very large data and dont want to wait for an accurate count.\n",
    "\n",
    "If you wanted to count the distinct elements you could use:\n",
    "\n",
    "- `countApproxDistinct`\n",
    "\n",
    "refer to documentation for more info on optional arguments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First\n",
    "\n",
    "Pulls the first element from your RDD. Nothing special here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1,6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Take\n",
    "\n",
    "Take # of elements from an RDD and returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,6,1))\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TakeSample\n",
    "\n",
    "Pulls a random sample of elements of a given size from the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 19, 4, 8, 17, 46, 0, 9]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(50))\n",
    "rdd.takeSample(withReplacement=False, num=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TakeOrdered\n",
    "\n",
    "Sorts the RDD, and then take a # of elements.\n",
    "\n",
    "Really fast when N is small - if you want to sort the entire RDD, just sort, no point in doing `takeOrdered`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### saveAsTextFile\n",
    "\n",
    "number of partitions will equal the number of output files.\n",
    "\n",
    "You should repartition before running `saveAsTextFile` in order to control number of output files!\n",
    "\n",
    "You can also use compression codecs as optional entries! Excellent for saving space on disk.\n",
    "\n",
    "In python - you can use `saveAsPickleFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(9999), numSlices=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd.saveAsTextFile(\"put_output_folder_name_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CountByKey\n",
    "\n",
    "Counts the occurence of Keys in an RDD.\n",
    "\n",
    "There is also a `countByValue` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('Andrew', 45), ('Juan', 99), ('Mauricio', 12), ('Mauricio', 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Andrew': 1, 'Juan': 1, 'Mauricio': 2})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ForEach\n",
    "\n",
    "Takes an action for each element of an RDD!\n",
    "\n",
    "Be sure to use reduce or an accumulator if you wish to see the results reflected in the driver.\n",
    "\n",
    "By default, sparks nodes do not push back variable changes to the driver.\n",
    "\n",
    "it's very important to distinguish between the local and cluster environments.\n",
    "\n",
    "Read this very important part of the documentation [here](https://spark.apache.org/docs/2.1.1/programming-guide.html#understanding-closures-a-nameclosureslinka)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's all I have for today. Plenty of information for you to play with and get your hands dirty!!\n",
    "\n",
    "The next notebook will cover more on key-value pairs, IO actions in addition to performance boosters! Stay tuned.\n",
    "\n",
    "As always, feel free to reach out: igabr@uchicago.edu or [@Gabr\\_Ibrahim](https://twitter.com/Gabr_Ibrahim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
