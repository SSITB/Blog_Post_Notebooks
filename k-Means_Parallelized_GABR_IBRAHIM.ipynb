{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version: 0.20.3\n",
      "NumPy Version: 1.13.1\n",
      "System Version: 3.6.2 |Anaconda custom (x86_64)| (default, Jul 20 2017, 13:14:59) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "print(f\"System Version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"County_Mortgage_Funding.csv\")\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_to_centroid(centroid_dictionary, classification_dictionary, data):\n",
    "    \"\"\"\n",
    "    Given a dictionary containing the positions of current centroids and a dictionary that will host the classifications of points to a centroid,\n",
    "    this function will assign points to a particular centroid using euclidean distance.\n",
    "    \n",
    "    This is the function that is executed in parallel.\n",
    "    \"\"\"\n",
    "    for point in data:\n",
    "        # list of length k\n",
    "        norms = [np.linalg.norm(point-centroid_dictionary[centroid]) for centroid in centroid_dictionary]\n",
    "        # index corresponds to cluster #\n",
    "        centroid_assignment = norms.index(min(norms))\n",
    "        classification_dictionary[centroid_assignment].append(point)\n",
    "    return classification_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel_kmeans(k, data, processors,iterations):\n",
    "    \"\"\"\n",
    "    This function will run the k-Means algorithm in parallel.\n",
    "    \n",
    "    Inputs:\n",
    "        k = Number of Clusters\n",
    "        data = numpy array\n",
    "        processors = number of cores to use\n",
    "        iterations = total iterations to run if algo does not converge.\n",
    "    Outputs:\n",
    "        merged_classification_dict = A dictionary containing the classification for each point in n-dimensional space.\n",
    "        centroid_dict = A dictionary contains the location of the k-centroids in n-dimensional space. \n",
    "    \"\"\"\n",
    "    seed = np.random.RandomState(21) #random seed\n",
    "    \n",
    "    # splitting the data according to number of processors requested.\n",
    "    data_split = np.array_split(data, processors)\n",
    "    \n",
    "    # this dictionary will store our centroids and their locations in n-dimensional space.\n",
    "    centroid_dict = {}\n",
    "    \n",
    "    all_indicies = np.arange(len(data)) #all possible indicies of the data matrix\n",
    "    \n",
    "    #pick k random starting indicies for centroids - no replacement, want unique indicies\n",
    "    intitial_centroids_indicies = seed.choice(all_indicies, size=k, replace=False)\n",
    "    \n",
    "    #setting k-random rows as initial centroids.\n",
    "    for i in range(k):\n",
    "        row = data[intitial_centroids_indicies[i]]\n",
    "        centroid_dict[i] = row\n",
    "\n",
    "    # this outer loop will ensure we stop after a certain number of iterations if we do not converge.\n",
    "    for iter_ in range(iterations):\n",
    "        classification_dict = {} # will store classifications of each n-dimensional point to a centroid\n",
    "\n",
    "        for i in range(k):\n",
    "            classification_dict[i] = [] #points to be stored in lists according to centroid.\n",
    "        \n",
    "        # multi-processing stage\n",
    "        with Pool(processes=processors) as pool: #context manager\n",
    "            process_objects = [pool.apply_async(assign_to_centroid, args=(centroid_dict, classification_dict, data_chunk)) for data_chunk in data_split]\n",
    "            process_results = [proc.get() for proc in process_objects]\n",
    "        \n",
    "        # take the first dictionary that contains centroid assignments for points\n",
    "        merged_classification_dict = process_results.pop(0)\n",
    "        \n",
    "        # Merging all the other dictionaries with the above dictionary.\n",
    "        for dictionary in process_results:\n",
    "            for key in dictionary:\n",
    "                merged_classification_dict[key].extend(dictionary[key])\n",
    "        \n",
    "        old_centroids = dict(centroid_dict) # deep copy and to keep track of old_centroid locations\n",
    "        \n",
    "        # Here we are updating the centroid positions\n",
    "        for class_ in merged_classification_dict:\n",
    "            centroid_dict[class_] = np.mean(merged_classification_dict[class_], axis=0)\n",
    "\n",
    "        convergence = True #Assume we have converged!\n",
    "\n",
    "        for centroid in centroid_dict:\n",
    "            previous_position = old_centroids[centroid]\n",
    "            new_position = centroid_dict[centroid]\n",
    "            # Calculated the distortion\n",
    "            distortion = np.linalg.norm(new_position-previous_position)\n",
    "            # if distortion is not 0,then we have not converged to a minimum. We need to keep iterating.\n",
    "            if distortion != 0:\n",
    "                convergence = False\n",
    "        if convergence: # if we converge, break out of the iteration loop\n",
    "#             print(f\"Convergence on iteration {iter_}\")\n",
    "            break\n",
    "    \n",
    "    return merged_classification_dict, centroid_dict\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "Note: In terms of performance, using `.apply_async` significantly outperformed `.apply`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.91 s ± 55.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=2, 1 processor, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(2, df.values, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11 s ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=2, 2 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(2, df.values, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99 s ± 196 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=2, 3 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(2, df.values, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.33 s ± 51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=2, 8 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(2, df.values, 8, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 s ± 585 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=3, 1 processor, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(3, df.values, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.52 s ± 301 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=3, 2 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(3, df.values, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.43 s ± 367 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=3, 3 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(3, df.values, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.51 s ± 582 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# k=3, 8 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(3, df.values, 8, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k=2, 8 processors, max of 1000 iterations\n",
    "classifications, centroids = parallel_kmeans(2, df.values, 8, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications.keys() # we have 2 centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(classifications[0]) + len(classifications[1])) == df.shape[0] # all rows classified to a centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([   85.90166694,    45.93417902,   133.72052453,   831.39791238,\n",
       "          220.61268692,  1653.34519324,   238.66021501,   119.17554623,\n",
       "          190.86753205,    76.74554451,    86.86499548,   278.95426686]),\n",
       " 1: array([  109.84707582,   104.03799988,   213.78882808,  1315.51916013,\n",
       "          244.13240171,  1559.10055202,  3184.36870797,  1652.7840744 ,\n",
       "         1649.7245409 ,  1783.84301673,  1235.71272533,  4047.75583568])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids # The final centroid positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
